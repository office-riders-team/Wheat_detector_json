{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q timm\n!pip install -q effdet\n!pip install -q py-cpuinfo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim import lr_scheduler\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport effdet\nimport timm\nimport cpuinfo\nimport gc\nimport wandb\nfrom warnings import simplefilter\n\n\nsimplefilter(\"ignore\")\nwandb.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_device_name(device):\n    def get_cpu():\n        cpu_info = cpuinfo.get_cpu_info()\n        device_name = cpu_info[\"brand_raw\"]\n        device_type = \"CPU\"\n        \n        return device_name, device_type\n    \n        \n    if \"cuda\" in device:\n        if torch.cuda.is_available():\n            device_name = torch.cuda.get_device_name(device)\n            device_type = \"GPU\"\n        else:\n            device_name, device_type = get_cpu()\n    else:\n        device_name, device_type = get_cpu()\n        \n    name = f\"{device_name} ({device_type})\"\n    return name\n\n\ndef transform_bounding_boxes(bounding_boxes, source_format=\"pascal_voc\", target_format=\"pascal_voc\"):\n    transformed_bounding_boxes = []\n    for bounding_box in bounding_boxes:\n        transformed_bounding_box = transform_bounding_box(bounding_box, source_format=source_format, target_format=target_format)\n        transformed_bounding_boxes.append(transformed_bounding_box)\n        \n    transformed_bounding_boxes = np.array(transformed_bounding_boxes)\n    return transformed_bounding_boxes\n        \n\ndef transform_bounding_box(bounding_box, source_format=\"pascal_voc\", target_format=\"pascal_voc\"):\n    methods = {\n        \"pascal_voc\": from_pascal_voc,\n        \"coco\": from_coco,\n        \"yolo\": from_yolo,\n    }\n    \n    from_method = methods.get(source_format, from_pascal_voc)\n        \n    transformed_bounding_box = from_method(bounding_box=bounding_box, target_format=target_format)\n        \n    return transformed_bounding_box\n        \n\ndef from_pascal_voc(bounding_box, target_format=\"pascal_voc\"):\n    x_min, y_min, x_max, y_max = bounding_box\n        \n    width = x_max - x_min\n    height = y_max - y_min\n        \n    half_width = width / 2\n    half_height = height / 2\n        \n    if target_format == \"coco\":\n        formated_bounding_box = [x_min, y_min, width, height]\n            \n    elif target_format == \"yolo\":\n        x_center = x_max / 2\n        y_center = y_max / 2\n            \n        formated_bounding_box = [x_center, y_center, width, height]\n            \n    else:\n        formated_bounding_box = bounding_box\n            \n    formated_bounding_box = np.array(formated_bounding_box).round()\n        \n    return formated_bounding_box\n        \ndef from_coco(bounding_box, target_format=\"pascal_voc\"):\n    x_min, y_min, width, height = bounding_box \n        \n    x_max = x_min + width\n    y_max = y_min + height\n        \n    if target_format == \"pascal_voc\":\n        formated_bounding_box = [x_min, y_min, x_max, y_max]\n            \n    elif target_format == \"yolo\":\n        x_center = x_max / 2\n        y_center = y_max / 2\n            \n        formated_bounding_box = [x_center, y_center, width, height]\n            \n    else:\n        formated_bounding_box = bounding_box\n            \n    formated_bounding_box = np.array(formated_bounding_box).round()\n        \n    return formated_bounding_box\n    \n\ndef from_yolo(bounding_box, target_format=\"pascal_voc\"):\n    x_center, y_center, width, height = bounding_box\n        \n    half_width = width / 2\n    half_height = height / 2\n        \n    x_max = x_center + half_width\n    x_min = x_center - half_width\n    y_max = y_center + half_height\n    y_min = y_center - half_height\n        \n    if target_format == \"pascal_voc\":\n        formated_bounding_box = [x_min, y_min, x_max, y_max]\n            \n    elif target_format == \"coco\":\n        formated_bounding_box = [x_min, y_min, width, height]\n        \n    else:\n        formated_bounding_box = bounding_box\n            \n    return formated_bounding_box\n\n\ndef draw_bboxes(image, bboxes, source_format=\"pascal_voc\", color=(0, 255, 255), thickness=1):\n    methods = {\n        \"pascal_voc\": from_pascal_voc,\n        \"coco\": from_coco,\n        \"yolo\": from_yolo,\n    }\n    \n    image_with_bboxes = image.copy()\n    for bbox in bboxes:\n        from_method = methods.get(source_format, from_pascal_voc)\n        bbox = from_method(bounding_box=bbox, target_format=\"pascal_voc\")\n        x_min, y_min, x_max, y_max = bbox.round().astype(int)\n        image_with_bboxes = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n        \n    return image_with_bboxes\n\n\ndef save_checkpoint(model, optimizer, epoch=None, loss=None, path=\"checkpoint.pth\"):\n    checkpoint = {\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"loss\": loss\n    }\n    \n    torch.save(checkpoint, path=path)\n    \n    return checkpoint\n\n\ndef create_model(model_name=\"tf_efficientdet_d0\", num_classes=1, pretrained=True, image_size=(512, 512), checkpoint_path=None, mode=\"train\"):\n    config = effdet.get_efficientdet_config(model_name)\n    config.image_size = image_size\n    config.num_classes = num_classes\n    config.norm_kwargs=dict(eps=.001, momentum=.01)\n\n    model = effdet.EfficientDet(config, pretrained_backbone=pretrained)\n    model.class_net = effdet.efficientdet.HeadNet(config, num_outputs=config.num_classes)\n\n    if checkpoint_path is not None:\n        if torch.cuda.is_available():\n            checkpoint = torch.load(checkpoint_path)\n        else:\n            checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n        \n        model.load_state_dict(checkpoint)            \n        print(f\"Loaded checkpoint from '{checkpoint_path}'\")\n    \n    \n    if mode == \"inference\":\n        model = effdet.DetBenchPredict(model, config)\n    else:\n        model = effdet.DetBenchTrain(model, config)\n        \n    return model\n\n\ndef train_one_batch(batch, model, optimizer, scaler=None, clipping_norm=None, inputs_device=\"cpu\", targets_device=\"cpu\"):\n    optimizer.zero_grad()\n    \n    if scaler is not None:\n        with autocast():\n            inputs, targets = Dataset.collate_batch(batch, inputs_device=inputs_device, targets_device=targets_device)\n            outputs = model(inputs, targets)\n\n            batch_loss = outputs[\"loss\"]\n\n        scaler.scale(batch_loss).backward()\n        \n        if clipping_norm is not None:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clipping_norm)\n        \n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        inputs, targets =  Dataset.collate_batch(batch, inputs_device=inputs_device, targets_device=targets_device)\n        outputs = model(inputs, targets)\n        \n        batch_loss = outputs[\"loss\"]\n        \n        batch_loss.backward()\n        if clipping_norm is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clipping_norm)\n            \n        optimizer.step()\n        \n    return batch_loss\n\n\ndef validate(model, loader, inputs_device=\"cpu\", targets_device=\"cpu\"):\n    loss = 0\n    with torch.no_grad():\n        for batch in loader:\n            inputs, targets = Dataset.collate_batch(batch, inputs_device=inputs_device, targets_device=targets_device)\n            outputs = model(inputs, targets)\n\n            batch_loss = outputs[\"loss\"]\n            loss += batch_loss\n        \n    loss /= len(loader)\n    \n    return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, pathes, bboxes=None, bboxes_format=\"pascal_voc\", bboxes_format_return=\"pascal_voc\", transforms=None):\n        self.pathes = pathes\n        self.bboxes = bboxes\n        self.transforms = transforms\n        self.bboxes_format = bboxes_format\n        self.bboxes_format_return = bboxes_format_return\n        \n    def __len__(self):\n        return len(self.pathes)\n    \n    @staticmethod\n    def get_bboxes(bboxes_string):\n        if bboxes_string != \"no_box\":\n            bboxes = bboxes_string.split(\";\")\n            new_bboxes = []\n            for bbox in bboxes:\n                new_bbox = bbox.split()\n                new_bboxes.append(new_bbox)\n        else:\n            new_bboxes = []\n\n        new_bboxes = np.asarray(new_bboxes, dtype=np.int32)\n        return new_bboxes\n    \n    def __getitem__(self, index):\n        path = self.pathes[index]\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        \n        if self.bboxes is not None:\n            bboxes = self.bboxes[index]\n            bboxes = Dataset.get_bboxes(bboxes)\n            labels = np.ones(shape=(bboxes.shape[0], ), dtype=np.int32)\n            \n            if self.transforms is not None:\n                height, width, channels = image.shape\n                bboxes = A.normalize_bbox(bboxes, cols=width, rows=height)\n                augmented = self.transforms(image=image, bboxes=bboxes, class_labels=labels, class_categories=labels)\n                image, bboxes = augmented[\"image\"], augmented[\"bboxes\"]\n                bboxes = transform_bounding_boxes(bboxes, source_format=self.bboxes_format, target_format=self.bboxes_format_return)\n        \n            labels = np.ones(shape=(bboxes.shape[0], ), dtype=np.int32)\n\n            image = torch.tensor(image)\n            bboxes = torch.tensor(bboxes)\n            labels = torch.tensor(labels)\n            \n            target = {\n                \"bboxes\": bboxes,\n                \"labels\": labels,\n            }\n            \n            return image, target\n        \n        return image\n    \n    \n    def show_samples(self, rows=1, columns=1, color=(0, 255, 255), thickness=1, coef=3):\n        fig = plt.figure(figsize=(columns*coef, rows*coef))\n        n_iterations = rows * columns\n        n_samples = len(self)\n        \n        for i in range(n_iterations):\n            index = random.randint(0, n_samples-1)\n            path = self.pathes[index]\n            image = cv2.imread(path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            \n            if self.bboxes is not None:\n                bboxes = self.bboxes[index]\n                bboxes = Dataset.get_bboxes(bboxes)\n                image = draw_bboxes(image, bboxes, source_format=\"pascal_voc\", color=color, thickness=thickness)\n                \n            ax = fig.add_subplot(rows, columns, i+1)\n            ax.xaxis.set_visible(False)\n            ax.yaxis.set_visible(False)\n            ax.set_title(f\"Sample's index: {index}\", loc=\"left\")\n            ax.imshow(image)\n        \n        fig.show()\n    \n    @staticmethod\n    def collate_batch(batch, inputs_device=\"cpu\", targets_device=\"cpu\"):\n        if len(batch) == 2:\n            inputs, targets = batch\n            inputs = inputs.to(inputs_device)\n            targets[\"bbox\"] = [bbox.to(targets_device).float() for bbox in targets[\"bbox\"]]\n            targets[\"cls\"] = [label.to(targets_device).float() for label in targets[\"cls\"]]\n            \n            return inputs, targets\n        else:\n            return batch.to(inputs_device)\n        \n    @staticmethod\n    def collate_fn(batch):\n        all_images, all_bboxes, all_labels = [], [], []\n        for sample in batch:\n            if len(sample) == 2:\n                image, target = sample\n                all_images.append(image.numpy())\n                \n                bboxes = target[\"bboxes\"]\n                all_bboxes.append(bboxes)\n                \n                labels = target[\"labels\"]\n                all_labels.append(labels)\n                \n            else:\n                image = sample\n                all_images.append(image.numpy())\n                \n        all_images = torch.tensor(all_images, dtype=torch.float32)\n        \n        if (len(all_bboxes) != 0) and (len(all_labels) != 0):\n            all_targets = {\n                \"bbox\": all_bboxes,\n                \"cls\": all_labels,\n            }\n        \n            return all_images, all_targets\n        \n        return all_images","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"epochs\": 1,\n    \"device\": \"cuda\",\n    \"size\": (512, 512),\n    \"lr\": 1e-3,\n    \"batch_size\": 4,\n    \"pin_memory\": True,\n    \"num_workers\": 4,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images_path = \"../input/wheat-detection/user_task/images\"\ntrain_path = \"../input/wheat-detection/user_task/train.csv\"\ntrain = pd.read_csv(train_path)\ntrain[\"image_name\"] = train[\"image_name\"].apply(lambda filename: os.path.join(train_images_path, filename))\ntrain.columns = [\"path\", \"bboxes\"]\n\nno_boxes = train[train[\"bboxes\"] == \"no_box\"]\ntrain = train.drop(no_boxes.index, axis=0)\n\ncrashed_samples = [267, 564, 695, 847, 870, 1059, 1160, 1286, 1296, 1609, 1618, 1644, 1720, 1764, 1921, 1963, 2088, 2134, 2309, 2327, 2476, 2533, 2613, 2621, 2633, 2653, 2676, 2689, 2701, 2731, 2814, 2816, 2897, 3090, 3368, 3436, 3581, 3587, 3688, 3717, 3778, 3853, 3858, 3974, 4145, 4236, 4381, 4407, 4410, 4444, 4514, 4583]\ntrain = train.drop(crashed_samples, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = 0.2\ntrain_data, validation_data, train_targets, validation_targets = train_test_split(train[\"path\"].values, train[\"bboxes\"].values, test_size=test_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"width, height = config[\"size\"]\n\nbbox_parameters = A.BboxParams(format=\"pascal_voc\", \n                               label_fields=['class_labels', 'class_categories'], \n                               min_area=0, \n                               min_visibility=0)\n\ntrain_transforms = A.Compose([\n    A.Resize(width=width, height=height),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.2),\n    A.Normalize(),\n    ToTensorV2(),\n], bbox_params=bbox_parameters)\n\ntrain_dataset = Dataset(pathes=train_data, \n                        bboxes=train_targets, \n                        bboxes_format=\"pascal_voc\", \n                        bboxes_format_return=\"pascal_voc\", \n                        transforms=train_transforms)\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=config[\"batch_size\"], \n                          shuffle=True, \n                          num_workers=config[\"num_workers\"], \n                          pin_memory=config[\"pin_memory\"], \n                          drop_last=False, \n                          collate_fn=Dataset.collate_fn)\n\n\n\nvalidation_transforms = A.Compose([\n    A.Resize(width=width, height=height),\n    A.Normalize(),\n    ToTensorV2(),\n],  bbox_params=bbox_parameters)\n\nvalidation_dataset = Dataset(pathes=validation_data, \n                             bboxes=validation_targets, \n                             bboxes_format=\"pascal_voc\", \n                             bboxes_format_return=\"pascal_voc\", \n                             transforms=validation_transforms)\n\nvalidation_loader = DataLoader(dataset=validation_dataset, \n                               batch_size=config[\"batch_size\"], \n                               shuffle=True, \n                               num_workers=config[\"num_workers\"], \n                               pin_memory=config[\"pin_memory\"], \n                               drop_last=False, \n                               collate_fn=Dataset.collate_fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.show_samples(rows=1, columns=1, thickness=2, coef=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(model_name=\"tf_efficientdet_d5\", \n                     pretrained=False, \n                     mode=\"train\", \n                     image_size=config[\"size\"],\n                     checkpoint_path=\"../input/wheat2020-checkpoints/effdet_ed5_512_fold4.pth\")\n\noptimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"])\nscaler = GradScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment_name = f\"EfficientDetD5\"\nnum_iterations_per_epoch = len(train_loader)\nnum_iterations = config[\"epochs\"] * num_iterations_per_epoch\ndevice_name = get_device_name(config[\"device\"])\ndebug_iterations = 50\nvalidation_steps = 500\nbest_validation_loss = np.inf\npassed_iterations = 1\n\nwandb.init(project=\"Wheat Detection\", entity=\"zzmtsvv\", name=experiment_name)\nmodel.to(DEVICE)\nprint(f\"Training on '{device_name}' for {num_iterations} iterations / {config['epochs']} epochs\")\nfor epoch in range(1, config[\"epochs\"]+1):\n    model.train()\n    for batch in train_loader:\n        batch_loss = train_one_batch(batch=batch, \n                                     model=model, \n                                     optimizer=optimizer, \n                                     scaler=scaler, \n                                     clipping_norm=1, \n                                     inputs_device=config[\"device\"], \n                                     targets_device=config[\"device\"])\n    \n        if passed_iterations % validation_steps == 0:\n            validation_loss = validate(model=model, \n                                       loader=validation_loader, \n                                       inputs_device=config[\"device\"], \n                                       targets_device=config[\"device\"])\n        \n            if validation_loss.item() < best_validation_loss:\n                checkpoint_path = f\"{experiment_name}_{validation_loss.item()}.pth\"\n                checkpoint = save_checkpoint(model=model, \n                                             optimizer=optimizer, \n                                             epoch=epoch, \n                                             loss=validation_loss.item(), \n                                             path=checkpoint_path)\n                \n                best_validation_loss = validation_loss\n                print(f\"Iteration [{passed_iterations}/{num_iterations}] Saving Checkpoint with Validation Loss: {validation_loss.item()} | Path: {checkpoint_path}\")\n            \n            \n            print(f\"Iteration [{passed_iterations}/{num_iterations}] Validation Loss: {validation_loss.item()}\")\n            wandb.log({\"validation_loss\": validation_loss.item()})\n        \n        if passed_iterations % debug_iterations == 0:\n            print(f\"Iteration [{passed_iterations}/{num_iterations}] Train Loss: {batch_loss.item()}\")\n            \n        wandb.log({\"train_loss\": batch_loss.item()})    \n        passed_iterations += 1","metadata":{},"execution_count":null,"outputs":[]}]}